{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EdgeNet, ObjectNet Confluenced",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pjdVhUccsEJ",
        "colab_type": "code",
        "outputId": "cc240fe8-6912-480a-ce11-c155f32ee370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "! git clone https://github.com/Nikronic/ObjectNet.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ObjectNet'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/18)   \u001b[K\rremote: Counting objects:  11% (2/18)   \u001b[K\rremote: Counting objects:  16% (3/18)   \u001b[K\rremote: Counting objects:  22% (4/18)   \u001b[K\rremote: Counting objects:  27% (5/18)   \u001b[K\rremote: Counting objects:  33% (6/18)   \u001b[K\rremote: Counting objects:  38% (7/18)   \u001b[K\rremote: Counting objects:  44% (8/18)   \u001b[K\rremote: Counting objects:  50% (9/18)   \u001b[K\rremote: Counting objects:  55% (10/18)   \u001b[K\rremote: Counting objects:  61% (11/18)   \u001b[K\rremote: Counting objects:  66% (12/18)   \u001b[K\rremote: Counting objects:  72% (13/18)   \u001b[K\rremote: Counting objects:  77% (14/18)   \u001b[K\rremote: Counting objects:  83% (15/18)   \u001b[K\rremote: Counting objects:  88% (16/18)   \u001b[K\rremote: Counting objects:  94% (17/18)   \u001b[K\rremote: Counting objects: 100% (18/18)   \u001b[K\rremote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 141 (delta 1), reused 17 (delta 0), pack-reused 123\u001b[K\n",
            "Receiving objects: 100% (141/141), 2.01 MiB | 5.17 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQx0A3iDGq4u",
        "colab_type": "code",
        "outputId": "f113d48f-1d9f-43e1-a2fc-c1e3aec6bb18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "! git clone https://github.com/Nikronic/CoarseNet.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CoarseNet'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects:   8% (1/12)   \u001b[K\rremote: Counting objects:  16% (2/12)   \u001b[K\rremote: Counting objects:  25% (3/12)   \u001b[K\rremote: Counting objects:  33% (4/12)   \u001b[K\rremote: Counting objects:  41% (5/12)   \u001b[K\rremote: Counting objects:  50% (6/12)   \u001b[K\rremote: Counting objects:  58% (7/12)   \u001b[K\rremote: Counting objects:  66% (8/12)   \u001b[K\rremote: Counting objects:  75% (9/12)   \u001b[K\rremote: Counting objects:  83% (10/12)   \u001b[K\rremote: Counting objects:  91% (11/12)   \u001b[K\rremote: Counting objects: 100% (12/12)   \u001b[K\rremote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects:   8% (1/12)   \u001b[K\rremote: Compressing objects:  16% (2/12)   \u001b[K\rremote: Compressing objects:  25% (3/12)   \u001b[K\rremote: Compressing objects:  33% (4/12)   \u001b[K\rremote: Compressing objects:  41% (5/12)   \u001b[K\rremote: Compressing objects:  50% (6/12)   \u001b[K\rremote: Compressing objects:  58% (7/12)   \u001b[K\rremote: Compressing objects:  66% (8/12)   \u001b[K\rremote: Compressing objects:  75% (9/12)   \u001b[K\rremote: Compressing objects:  83% (10/12)   \u001b[K\rremote: Compressing objects:  91% (11/12)   \u001b[K\rremote: Compressing objects: 100% (12/12)   \u001b[K\rremote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "Receiving objects:   0% (1/210)   \rReceiving objects:   1% (3/210)   \rReceiving objects:   2% (5/210)   \rReceiving objects:   3% (7/210)   \rReceiving objects:   4% (9/210)   \rReceiving objects:   5% (11/210)   \rReceiving objects:   6% (13/210)   \rReceiving objects:   7% (15/210)   \rReceiving objects:   8% (17/210)   \rReceiving objects:   9% (19/210)   \rReceiving objects:  10% (21/210)   \rReceiving objects:  11% (24/210)   \rReceiving objects:  12% (26/210)   \rReceiving objects:  13% (28/210)   \rReceiving objects:  14% (30/210)   \rReceiving objects:  15% (32/210)   \rReceiving objects:  16% (34/210)   \rReceiving objects:  17% (36/210)   \rReceiving objects:  18% (38/210)   \rReceiving objects:  19% (40/210)   \rReceiving objects:  20% (42/210)   \rReceiving objects:  21% (45/210)   \rReceiving objects:  22% (47/210)   \rReceiving objects:  23% (49/210)   \rReceiving objects:  24% (51/210)   \rReceiving objects:  25% (53/210)   \rReceiving objects:  26% (55/210)   \rReceiving objects:  27% (57/210)   \rReceiving objects:  28% (59/210)   \rReceiving objects:  29% (61/210)   \rReceiving objects:  30% (63/210)   \rReceiving objects:  31% (66/210)   \rReceiving objects:  32% (68/210)   \rremote: Total 210 (delta 3), reused 6 (delta 0), pack-reused 198\u001b[K\n",
            "Receiving objects:  33% (70/210)   \rReceiving objects:  34% (72/210)   \rReceiving objects:  35% (74/210)   \rReceiving objects:  36% (76/210)   \rReceiving objects:  37% (78/210)   \rReceiving objects:  38% (80/210)   \rReceiving objects:  39% (82/210)   \rReceiving objects:  40% (84/210)   \rReceiving objects:  41% (87/210)   \rReceiving objects:  42% (89/210)   \rReceiving objects:  43% (91/210)   \rReceiving objects:  44% (93/210)   \rReceiving objects:  45% (95/210)   \rReceiving objects:  46% (97/210)   \rReceiving objects:  47% (99/210)   \rReceiving objects:  48% (101/210)   \rReceiving objects:  49% (103/210)   \rReceiving objects:  50% (105/210)   \rReceiving objects:  51% (108/210)   \rReceiving objects:  52% (110/210)   \rReceiving objects:  53% (112/210)   \rReceiving objects:  54% (114/210)   \rReceiving objects:  55% (116/210)   \rReceiving objects:  56% (118/210)   \rReceiving objects:  57% (120/210)   \rReceiving objects:  58% (122/210)   \rReceiving objects:  59% (124/210)   \rReceiving objects:  60% (126/210)   \rReceiving objects:  61% (129/210)   \rReceiving objects:  62% (131/210)   \rReceiving objects:  63% (133/210)   \rReceiving objects:  64% (135/210)   \rReceiving objects:  65% (137/210)   \rReceiving objects:  66% (139/210)   \rReceiving objects:  67% (141/210)   \rReceiving objects:  68% (143/210)   \rReceiving objects:  69% (145/210)   \rReceiving objects:  70% (147/210)   \rReceiving objects:  71% (150/210)   \rReceiving objects:  72% (152/210)   \rReceiving objects:  73% (154/210)   \rReceiving objects:  74% (156/210)   \rReceiving objects:  75% (158/210)   \rReceiving objects:  76% (160/210)   \rReceiving objects:  77% (162/210)   \rReceiving objects:  78% (164/210)   \rReceiving objects:  79% (166/210)   \rReceiving objects:  80% (168/210)   \rReceiving objects:  81% (171/210)   \rReceiving objects:  82% (173/210)   \rReceiving objects:  83% (175/210)   \rReceiving objects:  84% (177/210)   \rReceiving objects:  85% (179/210)   \rReceiving objects:  86% (181/210)   \rReceiving objects:  87% (183/210)   \rReceiving objects:  88% (185/210)   \rReceiving objects:  89% (187/210)   \rReceiving objects:  90% (189/210)   \rReceiving objects:  91% (192/210)   \rReceiving objects:  92% (194/210)   \rReceiving objects:  93% (196/210)   \rReceiving objects:  94% (198/210)   \rReceiving objects:  95% (200/210)   \rReceiving objects:  96% (202/210)   \rReceiving objects:  97% (204/210)   \rReceiving objects:  98% (206/210)   \rReceiving objects:  99% (208/210)   \rReceiving objects: 100% (210/210)   \rReceiving objects: 100% (210/210), 177.46 KiB | 1.42 MiB/s, done.\n",
            "Resolving deltas:   0% (0/119)   \rResolving deltas:  23% (28/119)   \rResolving deltas:  30% (36/119)   \rResolving deltas:  35% (42/119)   \rResolving deltas:  65% (78/119)   \rResolving deltas:  66% (79/119)   \rResolving deltas:  68% (82/119)   \rResolving deltas:  69% (83/119)   \rResolving deltas:  71% (85/119)   \rResolving deltas:  75% (90/119)   \rResolving deltas:  77% (92/119)   \rResolving deltas:  78% (94/119)   \rResolving deltas:  79% (95/119)   \rResolving deltas:  84% (101/119)   \rResolving deltas:  96% (115/119)   \rResolving deltas:  97% (116/119)   \rResolving deltas:  98% (117/119)   \rResolving deltas:  99% (118/119)   \rResolving deltas: 100% (119/119)   \rResolving deltas: 100% (119/119), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igG6DhnfZq6f",
        "colab_type": "code",
        "outputId": "80776b3a-9962-4a56-9060-ee7d6aec2fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "! git clone https://github.com/Nikronic/EdgeNet.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EdgeNet'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/98)   \u001b[K\rremote: Counting objects:   2% (2/98)   \u001b[K\rremote: Counting objects:   3% (3/98)   \u001b[K\rremote: Counting objects:   4% (4/98)   \u001b[K\rremote: Counting objects:   5% (5/98)   \u001b[K\rremote: Counting objects:   6% (6/98)   \u001b[K\rremote: Counting objects:   7% (7/98)   \u001b[K\rremote: Counting objects:   8% (8/98)   \u001b[K\rremote: Counting objects:   9% (9/98)   \u001b[K\rremote: Counting objects:  10% (10/98)   \u001b[K\rremote: Counting objects:  11% (11/98)   \u001b[K\rremote: Counting objects:  12% (12/98)   \u001b[K\rremote: Counting objects:  13% (13/98)   \u001b[K\rremote: Counting objects:  14% (14/98)   \u001b[K\rremote: Counting objects:  15% (15/98)   \u001b[K\rremote: Counting objects:  16% (16/98)   \u001b[K\rremote: Counting objects:  17% (17/98)   \u001b[K\rremote: Counting objects:  18% (18/98)   \u001b[K\rremote: Counting objects:  19% (19/98)   \u001b[K\rremote: Counting objects:  20% (20/98)   \u001b[K\rremote: Counting objects:  21% (21/98)   \u001b[K\rremote: Counting objects:  22% (22/98)   \u001b[K\rremote: Counting objects:  23% (23/98)   \u001b[K\rremote: Counting objects:  24% (24/98)   \u001b[K\rremote: Counting objects:  25% (25/98)   \u001b[K\rremote: Counting objects:  26% (26/98)   \u001b[K\rremote: Counting objects:  27% (27/98)   \u001b[K\rremote: Counting objects:  28% (28/98)   \u001b[K\rremote: Counting objects:  29% (29/98)   \u001b[K\rremote: Counting objects:  30% (30/98)   \u001b[K\rremote: Counting objects:  31% (31/98)   \u001b[K\rremote: Counting objects:  32% (32/98)   \u001b[K\rremote: Counting objects:  33% (33/98)   \u001b[K\rremote: Counting objects:  34% (34/98)   \u001b[K\rremote: Counting objects:  35% (35/98)   \u001b[K\rremote: Counting objects:  36% (36/98)   \u001b[K\rremote: Counting objects:  37% (37/98)   \u001b[K\rremote: Counting objects:  38% (38/98)   \u001b[K\rremote: Counting objects:  39% (39/98)   \u001b[K\rremote: Counting objects:  40% (40/98)   \u001b[K\rremote: Counting objects:  41% (41/98)   \u001b[K\rremote: Counting objects:  42% (42/98)   \u001b[K\rremote: Counting objects:  43% (43/98)   \u001b[K\rremote: Counting objects:  44% (44/98)   \u001b[K\rremote: Counting objects:  45% (45/98)   \u001b[K\rremote: Counting objects:  46% (46/98)   \u001b[K\rremote: Counting objects:  47% (47/98)   \u001b[K\rremote: Counting objects:  48% (48/98)   \u001b[K\rremote: Counting objects:  50% (49/98)   \u001b[K\rremote: Counting objects:  51% (50/98)   \u001b[K\rremote: Counting objects:  52% (51/98)   \u001b[K\rremote: Counting objects:  53% (52/98)   \u001b[K\rremote: Counting objects:  54% (53/98)   \u001b[K\rremote: Counting objects:  55% (54/98)   \u001b[K\rremote: Counting objects:  56% (55/98)   \u001b[K\rremote: Counting objects:  57% (56/98)   \u001b[K\rremote: Counting objects:  58% (57/98)   \u001b[K\rremote: Counting objects:  59% (58/98)   \u001b[K\rremote: Counting objects:  60% (59/98)   \u001b[K\rremote: Counting objects:  61% (60/98)   \u001b[K\rremote: Counting objects:  62% (61/98)   \u001b[K\rremote: Counting objects:  63% (62/98)   \u001b[K\rremote: Counting objects:  64% (63/98)   \u001b[K\rremote: Counting objects:  65% (64/98)   \u001b[K\rremote: Counting objects:  66% (65/98)   \u001b[K\rremote: Counting objects:  67% (66/98)   \u001b[K\rremote: Counting objects:  68% (67/98)   \u001b[K\rremote: Counting objects:  69% (68/98)   \u001b[K\rremote: Counting objects:  70% (69/98)   \u001b[K\rremote: Counting objects:  71% (70/98)   \u001b[K\rremote: Counting objects:  72% (71/98)   \u001b[K\rremote: Counting objects:  73% (72/98)   \u001b[K\rremote: Counting objects:  74% (73/98)   \u001b[K\rremote: Counting objects:  75% (74/98)   \u001b[K\rremote: Counting objects:  76% (75/98)   \u001b[K\rremote: Counting objects:  77% (76/98)   \u001b[K\rremote: Counting objects:  78% (77/98)   \u001b[K\rremote: Counting objects:  79% (78/98)   \u001b[K\rremote: Counting objects:  80% (79/98)   \u001b[K\rremote: Counting objects:  81% (80/98)   \u001b[K\rremote: Counting objects:  82% (81/98)   \u001b[K\rremote: Counting objects:  83% (82/98)   \u001b[K\rremote: Counting objects:  84% (83/98)   \u001b[K\rremote: Counting objects:  85% (84/98)   \u001b[K\rremote: Counting objects:  86% (85/98)   \u001b[K\rremote: Counting objects:  87% (86/98)   \u001b[K\rremote: Counting objects:  88% (87/98)   \u001b[K\rremote: Counting objects:  89% (88/98)   \u001b[K\rremote: Counting objects:  90% (89/98)   \u001b[K\rremote: Counting objects:  91% (90/98)   \u001b[K\rremote: Counting objects:  92% (91/98)   \u001b[K\rremote: Counting objects:  93% (92/98)   \u001b[K\rremote: Counting objects:  94% (93/98)   \u001b[K\rremote: Counting objects:  95% (94/98)   \u001b[K\rremote: Counting objects:  96% (95/98)   \u001b[K\rremote: Counting objects:  97% (96/98)   \u001b[K\rremote: Counting objects:  98% (97/98)   \u001b[K\rremote: Counting objects: 100% (98/98)   \u001b[K\rremote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/73)   \u001b[K\rremote: Compressing objects:   2% (2/73)   \u001b[K\rremote: Compressing objects:   4% (3/73)   \u001b[K\rremote: Compressing objects:   5% (4/73)   \u001b[K\rremote: Compressing objects:   6% (5/73)   \u001b[K\rremote: Compressing objects:   8% (6/73)   \u001b[K\rremote: Compressing objects:   9% (7/73)   \u001b[K\rremote: Compressing objects:  10% (8/73)   \u001b[K\rremote: Compressing objects:  12% (9/73)   \u001b[K\rremote: Compressing objects:  13% (10/73)   \u001b[K\rremote: Compressing objects:  15% (11/73)   \u001b[K\rremote: Compressing objects:  16% (12/73)   \u001b[K\rremote: Compressing objects:  17% (13/73)   \u001b[K\rremote: Compressing objects:  19% (14/73)   \u001b[K\rremote: Compressing objects:  20% (15/73)   \u001b[K\rremote: Compressing objects:  21% (16/73)   \u001b[K\rremote: Compressing objects:  23% (17/73)   \u001b[K\rremote: Compressing objects:  24% (18/73)   \u001b[K\rremote: Compressing objects:  26% (19/73)   \u001b[K\rremote: Compressing objects:  27% (20/73)   \u001b[K\rremote: Compressing objects:  28% (21/73)   \u001b[K\rremote: Compressing objects:  30% (22/73)   \u001b[K\rremote: Compressing objects:  31% (23/73)   \u001b[K\rremote: Compressing objects:  32% (24/73)   \u001b[K\rremote: Compressing objects:  34% (25/73)   \u001b[K\rremote: Compressing objects:  35% (26/73)   \u001b[K\rremote: Compressing objects:  36% (27/73)   \u001b[K\rremote: Compressing objects:  38% (28/73)   \u001b[K\rremote: Compressing objects:  39% (29/73)   \u001b[K\rremote: Compressing objects:  41% (30/73)   \u001b[K\rremote: Compressing objects:  42% (31/73)   \u001b[K\rremote: Compressing objects:  43% (32/73)   \u001b[K\rremote: Compressing objects:  45% (33/73)   \u001b[K\rremote: Compressing objects:  46% (34/73)   \u001b[K\rremote: Compressing objects:  47% (35/73)   \u001b[K\rremote: Compressing objects:  49% (36/73)   \u001b[K\rremote: Compressing objects:  50% (37/73)   \u001b[K\rremote: Compressing objects:  52% (38/73)   \u001b[K\rremote: Compressing objects:  53% (39/73)   \u001b[K\rremote: Compressing objects:  54% (40/73)   \u001b[K\rremote: Compressing objects:  56% (41/73)   \u001b[K\rremote: Compressing objects:  57% (42/73)   \u001b[K\rremote: Compressing objects:  58% (43/73)   \u001b[K\rremote: Compressing objects:  60% (44/73)   \u001b[K\rremote: Compressing objects:  61% (45/73)   \u001b[K\rremote: Compressing objects:  63% (46/73)   \u001b[K\rremote: Compressing objects:  64% (47/73)   \u001b[K\rremote: Compressing objects:  65% (48/73)   \u001b[K\rremote: Compressing objects:  67% (49/73)   \u001b[K\rremote: Compressing objects:  68% (50/73)   \u001b[K\rremote: Compressing objects:  69% (51/73)   \u001b[K\rremote: Compressing objects:  71% (52/73)   \u001b[K\rremote: Compressing objects:  72% (53/73)   \u001b[K\rremote: Compressing objects:  73% (54/73)   \u001b[K\rremote: Compressing objects:  75% (55/73)   \u001b[K\rremote: Compressing objects:  76% (56/73)   \u001b[K\rremote: Compressing objects:  78% (57/73)   \u001b[K\rremote: Compressing objects:  79% (58/73)   \u001b[K\rremote: Compressing objects:  80% (59/73)   \u001b[K\rremote: Compressing objects:  82% (60/73)   \u001b[K\rremote: Compressing objects:  83% (61/73)   \u001b[K\rremote: Compressing objects:  84% (62/73)   \u001b[K\rremote: Compressing objects:  86% (63/73)   \u001b[K\rremote: Compressing objects:  87% (64/73)   \u001b[K\rremote: Compressing objects:  89% (65/73)   \u001b[K\rremote: Compressing objects:  90% (66/73)   \u001b[K\rremote: Compressing objects:  91% (67/73)   \u001b[K\rremote: Compressing objects:  93% (68/73)   \u001b[K\rremote: Compressing objects:  94% (69/73)   \u001b[K\rremote: Compressing objects:  95% (70/73)   \u001b[K\rremote: Compressing objects:  97% (71/73)   \u001b[K\rremote: Compressing objects:  98% (72/73)   \u001b[K\rremote: Compressing objects: 100% (73/73)   \u001b[K\rremote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "Unpacking objects:   1% (1/98)   \rUnpacking objects:   2% (2/98)   \rUnpacking objects:   3% (3/98)   \rUnpacking objects:   4% (4/98)   \rUnpacking objects:   5% (5/98)   \rUnpacking objects:   6% (6/98)   \rUnpacking objects:   7% (7/98)   \rUnpacking objects:   8% (8/98)   \rUnpacking objects:   9% (9/98)   \rUnpacking objects:  10% (10/98)   \rUnpacking objects:  11% (11/98)   \rUnpacking objects:  12% (12/98)   \rUnpacking objects:  13% (13/98)   \rUnpacking objects:  14% (14/98)   \rUnpacking objects:  15% (15/98)   \rUnpacking objects:  16% (16/98)   \rUnpacking objects:  17% (17/98)   \rUnpacking objects:  18% (18/98)   \rUnpacking objects:  19% (19/98)   \rUnpacking objects:  20% (20/98)   \rUnpacking objects:  21% (21/98)   \rUnpacking objects:  22% (22/98)   \rUnpacking objects:  23% (23/98)   \rUnpacking objects:  24% (24/98)   \rUnpacking objects:  25% (25/98)   \rUnpacking objects:  26% (26/98)   \rUnpacking objects:  27% (27/98)   \rUnpacking objects:  28% (28/98)   \rUnpacking objects:  29% (29/98)   \rUnpacking objects:  30% (30/98)   \rUnpacking objects:  31% (31/98)   \rUnpacking objects:  32% (32/98)   \rUnpacking objects:  33% (33/98)   \rUnpacking objects:  34% (34/98)   \rUnpacking objects:  35% (35/98)   \rUnpacking objects:  36% (36/98)   \rUnpacking objects:  37% (37/98)   \rUnpacking objects:  38% (38/98)   \rUnpacking objects:  39% (39/98)   \rUnpacking objects:  40% (40/98)   \rUnpacking objects:  41% (41/98)   \rUnpacking objects:  42% (42/98)   \rUnpacking objects:  43% (43/98)   \rremote: Total 98 (delta 54), reused 63 (delta 22), pack-reused 0\u001b[K\n",
            "Unpacking objects:  44% (44/98)   \rUnpacking objects:  45% (45/98)   \rUnpacking objects:  46% (46/98)   \rUnpacking objects:  47% (47/98)   \rUnpacking objects:  48% (48/98)   \rUnpacking objects:  50% (49/98)   \rUnpacking objects:  51% (50/98)   \rUnpacking objects:  52% (51/98)   \rUnpacking objects:  53% (52/98)   \rUnpacking objects:  54% (53/98)   \rUnpacking objects:  55% (54/98)   \rUnpacking objects:  56% (55/98)   \rUnpacking objects:  57% (56/98)   \rUnpacking objects:  58% (57/98)   \rUnpacking objects:  59% (58/98)   \rUnpacking objects:  60% (59/98)   \rUnpacking objects:  61% (60/98)   \rUnpacking objects:  62% (61/98)   \rUnpacking objects:  63% (62/98)   \rUnpacking objects:  64% (63/98)   \rUnpacking objects:  65% (64/98)   \rUnpacking objects:  66% (65/98)   \rUnpacking objects:  67% (66/98)   \rUnpacking objects:  68% (67/98)   \rUnpacking objects:  69% (68/98)   \rUnpacking objects:  70% (69/98)   \rUnpacking objects:  71% (70/98)   \rUnpacking objects:  72% (71/98)   \rUnpacking objects:  73% (72/98)   \rUnpacking objects:  74% (73/98)   \rUnpacking objects:  75% (74/98)   \rUnpacking objects:  76% (75/98)   \rUnpacking objects:  77% (76/98)   \rUnpacking objects:  78% (77/98)   \rUnpacking objects:  79% (78/98)   \rUnpacking objects:  80% (79/98)   \rUnpacking objects:  81% (80/98)   \rUnpacking objects:  82% (81/98)   \rUnpacking objects:  83% (82/98)   \rUnpacking objects:  84% (83/98)   \rUnpacking objects:  85% (84/98)   \rUnpacking objects:  86% (85/98)   \rUnpacking objects:  87% (86/98)   \rUnpacking objects:  88% (87/98)   \rUnpacking objects:  89% (88/98)   \rUnpacking objects:  90% (89/98)   \rUnpacking objects:  91% (90/98)   \rUnpacking objects:  92% (91/98)   \rUnpacking objects:  93% (92/98)   \rUnpacking objects:  94% (93/98)   \rUnpacking objects:  95% (94/98)   \rUnpacking objects:  96% (95/98)   \rUnpacking objects:  97% (96/98)   \rUnpacking objects:  98% (97/98)   \rUnpacking objects: 100% (98/98)   \rUnpacking objects: 100% (98/98), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuU-22bVCS9S",
        "colab_type": "code",
        "outputId": "34799019-a33a-4454-e56a-471e9bb7bb1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd ObjectNet/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ObjectNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY20eU3I5z8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "pwd\n",
        "rm -r ObjectNet/\n",
        "rm -r EdgeNet/\n",
        "rm -r sample_data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZDGTTZ7BRZ",
        "colab_type": "code",
        "outputId": "085e5885-4047-46e5-e0ad-a8e62941897b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/ObjectNet'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P85rMZuIjryX",
        "colab_type": "text"
      },
      "source": [
        "### Download pretrained weights of ObjectNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM2ZLgsajubU",
        "colab_type": "code",
        "outputId": "e21f72ce-7dbf-4542-bc0e-2393f4f8aa93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "%%bash\n",
        "\n",
        "# Image and model names\n",
        "TEST_IMG=ADE_val_00001519.jpg\n",
        "MODEL_PATH=baseline-resnet101dilated-ppm_deepsup\n",
        "RESULT_PATH=./\n",
        "\n",
        "ENCODER=$MODEL_PATH/encoder_epoch_25.pth\n",
        "DECODER=$MODEL_PATH/decoder_epoch_25.pth\n",
        "\n",
        "# Download model weights and image\n",
        "if [ ! -e $MODEL_PATH ]; then\n",
        "  mkdir $MODEL_PATH\n",
        "fi\n",
        "if [ ! -e $ENCODER ]; then\n",
        "  wget -P $MODEL_PATH http://sceneparsing.csail.mit.edu/model/pytorch/$ENCODER\n",
        "fi\n",
        "if [ ! -e $DECODER ]; then\n",
        "  wget -P $MODEL_PATH http://sceneparsing.csail.mit.edu/model/pytorch/$DECODER\n",
        "fi\n",
        "if [ ! -e $TEST_IMG ]; then\n",
        "  wget -P $RESULT_PATH http://sceneparsing.csail.mit.edu/data/ADEChallengeData2016/images/validation/$TEST_IMG\n",
        "fi\n",
        "\n",
        "# Inference\n",
        "python3 -u test.py \\\n",
        "  --model_path $MODEL_PATH \\\n",
        "  --test_imgs $TEST_IMG \\\n",
        "  --suffix '_epoch_25.pth' \\\n",
        "  --arch_encoder resnet101dilated \\\n",
        "  --arch_decoder ppm_deepsup \\\n",
        "  --fc_dim 2048 \\\n",
        "  --result $RESULT_PATH\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input arguments:\n",
            "test_imgs        ['ADE_val_00001519.jpg']\n",
            "model_path       baseline-resnet101dilated-ppm_deepsup\n",
            "suffix           _epoch_25.pth\n",
            "arch_encoder     resnet101dilated\n",
            "arch_decoder     ppm_deepsup\n",
            "fc_dim           2048\n",
            "num_val          -1\n",
            "num_class        150\n",
            "batch_size       1\n",
            "imgSize          [300, 400, 500, 600]\n",
            "imgMaxSize       1000\n",
            "padding_constant 8\n",
            "segm_downsampling_rate 8\n",
            "result           ./\n",
            "gpu              0\n",
            "Loading weights for net_encoder\n",
            "Loading weights for net_decoder\n",
            "# samples: 1\n",
            "Inference done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hqdzRoOdgTC",
        "colab_type": "text"
      },
      "source": [
        "# TRAIN.py  - - - - - - - - - - - - - - - EDGE NET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma747sxEdj-3",
        "colab_type": "code",
        "outputId": "d24cee3a-59f8-4b67-d06d-98b5ed734207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile train_edited.py\n",
        "\n",
        "# %% import library\n",
        "from EdgeNet import EdgeNet\n",
        "from torchvision.transforms import Compose, ToPILImage, ToTensor, RandomResizedCrop, RandomRotation, \\\n",
        "    RandomHorizontalFlip\n",
        "from EdgeNet.utils.preprocess import *\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from EdgeNet.utils.loss import EdgeLoss\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.backends import cudnn\n",
        "\n",
        "\n",
        "###########################\n",
        "# System libs\n",
        "import os\n",
        "import argparse\n",
        "from distutils.version import LooseVersion\n",
        "# Numerical libs\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy.io import loadmat\n",
        "# Our libs\n",
        "\n",
        "from models import ModelBuilder, SegmentationModule\n",
        "from utils import colorEncode\n",
        "from lib.nn import user_scattered_collate, async_copy_to\n",
        "from lib.utils import as_numpy\n",
        "import lib.utils.data as torchdata\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.utils\n",
        "# import argparse\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# %% config parser\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--txt\", help='path to the text file', default='filelist.txt')\n",
        "# parser.add_argument(\"--img\", help='path to the images tar archive (uncompressed) or extracted folder', default='data')\n",
        "# parser.add_argument(\"--txt_t\", help='path to the text file of test set', default='filelist.txt')\n",
        "# parser.add_argument(\"--img_t\", help='path to the images tar archive (uncompressed) or extracted folder of test set',\n",
        "#                     default='data')\n",
        "# parser.add_argument(\"--bs\", help='int number as batch size', default=128, type=int)\n",
        "# parser.add_argument(\"--es\", help='int number as number of epochs', default=10, type=int)\n",
        "# parser.add_argument(\"--nw\", help='number of workers (1 to 8 recommended)', default=4, type=int)\n",
        "# parser.add_argument(\"--lr\", help='learning rate of optimizer (=0.0001)', default=0.0001, type=float)\n",
        "# parser.add_argument(\"--cudnn\", help='enable(1) cudnn.benchmark or not(0)', default=0, type=int)\n",
        "# parser.add_argument(\"--pm\", help='enable(1) pin_memory or not(0)', default=0, type=int)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# cudnn.benchmark = False\n",
        "# pin_memory = False\n",
        "# cudnn.benchmark = True if args.cudnn == 1 else False\n",
        "# pin_memory = True if args.cudnn == 1 else False\n",
        "\n",
        "\n",
        "# %% define data sets and their loaders\n",
        "custom_transforms = Compose([\n",
        "    RandomResizedCrop(size=224, scale=(0.8, 1.2)),\n",
        "    RandomRotation(degrees=(-30, 30)),\n",
        "    RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    RandomNoise(p=0.5, mean=0, std=0.1)])\n",
        "\n",
        "train_dataset = PlacesDataset(txt_path='EdgeNet/filelist.txt',\n",
        "                              img_dir='EdgeNet/data',\n",
        "                              transform=custom_transforms)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=128,\n",
        "                          shuffle=True,\n",
        "                          num_workers=6,\n",
        "                          pin_memory= False)\n",
        "\n",
        "test_dataset = PlacesDataset(txt_path='EdgeNet/filelist.txt',\n",
        "                             img_dir='EdgeNet/data',\n",
        "                             transform=ToTensor())\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=128,\n",
        "                         shuffle=False,\n",
        "                         num_workers=0,\n",
        "                         pin_memory=False)\n",
        "\n",
        "\n",
        "# %% initialize network, loss and optimizer\n",
        "def init_weights(m):\n",
        "    \"\"\"\n",
        "    Initialize weights of layers using Kaiming Normal (He et al.) as argument of \"Apply\" function of\n",
        "    \"nn.Module\"\n",
        "\n",
        "    :param m: Layer to initialize\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
        "        m.bias.data.fill_(0.0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):  # reference: https://github.com/pytorch/pytorch/issues/12259\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "# %% train model\n",
        "def train_model(net, data_loader, optimizer, criterion, epochs=10):\n",
        "    \"\"\"\n",
        "    Train model\n",
        "\n",
        "    :param net: Parameters of defined neural network\n",
        "    :param data_loader: A data loader object defined on train data set\n",
        "    :param epochs: Number of epochs to train model\n",
        "    :param optimizer: Optimizer to train network\n",
        "    :param criterion: The loss function to minimize by optimizer\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    edgenet = net[0]\n",
        "    objectnet = net[1]\n",
        "    \n",
        "    edgenet.train()\n",
        "    objectnet.eval()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "            # get the inputs\n",
        "            y_descreen = data['y_descreen']\n",
        "            y_e = data['y_edge']\n",
        "\n",
        "            y_descreen = y_descreen.to(device)\n",
        "            y_e = y_e.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = edgenet(y_descreen)  # output of model to feed to concatnation\n",
        "            \n",
        "            outputs_obj = objectnet(y_descreen)\n",
        "            \n",
        "            loss = criterion(outputs, y_e.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss))\n",
        "            running_loss = 0.0\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "# %% test\n",
        "def test_model(net, data_loader):\n",
        "    \"\"\"\n",
        "    Return loss on test\n",
        "\n",
        "    :param net: The trained NN network\n",
        "    :param data_loader: Data loader containing test set\n",
        "    :return: Print loss value over test set in console\n",
        "    \"\"\"\n",
        "\n",
        "    edgenet = net[0]\n",
        "    objectnet = net[1]\n",
        "    \n",
        "    edgenet.train()\n",
        "    objectnet.eval()\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            y_descreen = data['y_descreen']\n",
        "            y_e = data['y_edge']\n",
        "\n",
        "            y_descreen = y_descreen.to(device)\n",
        "            y_e = y_e.to(device)\n",
        "            outputs = edgenet(y_descreen)\n",
        "            outputs_obj = objectnet(y_descreen) #####\n",
        "            loss = criterion(outputs, y_e)\n",
        "            running_loss += loss\n",
        "            print('loss: %.3f' % running_loss)\n",
        "    return outputs, outputs_obj\n",
        "\n",
        "\n",
        "def show_test(image_batch):\n",
        "    \"\"\"\n",
        "    Show a sample grid image which contains some sample of test set result\n",
        "\n",
        "    :param image_batch: The output batch of test set\n",
        "    :return: PIL image of all images of the input batch\n",
        "    \"\"\"\n",
        "\n",
        "    to_pil = ToPILImage()\n",
        "    fs = []\n",
        "    for i in range(len(image_batch)):\n",
        "        img = to_pil(image_batch[i].cpu())\n",
        "        fs.append(img)\n",
        "    x, y = fs[0].size\n",
        "    ncol = 3\n",
        "    nrow = 3\n",
        "    cvs = Image.new('RGB', (x * ncol, y * nrow))\n",
        "    for i in range(len(fs)):\n",
        "        px, py = x * int(i / nrow), y * (i % nrow)\n",
        "        cvs.paste((fs[i]), (px, py))\n",
        "    cvs.save('out.png', format='png')\n",
        "    cvs.show()\n",
        "\n",
        "\n",
        "# %% run model\n",
        "criterion = EdgeLoss()\n",
        "edgenet = EdgeNet.EdgeNet().to(device)\n",
        "\n",
        "builder = ModelBuilder()\n",
        "net_encoder = builder.build_encoder(\n",
        "    arch='resnet101dilated',\n",
        "    fc_dim=2048,\n",
        "    weights=os.path.join('baseline-resnet101dilated-ppm_deepsup', 'encoder' + '_epoch_25.pth'))\n",
        "net_decoder = builder.build_decoder(\n",
        "    arch='ppm_deepsup',\n",
        "    fc_dim=2048,\n",
        "    num_class=150,\n",
        "    weights=os.path.join('baseline-resnet101dilated-ppm_deepsup', 'decoder' + '_epoch_25.pth'),\n",
        "    use_softmax=True)\n",
        "\n",
        "    \n",
        "\n",
        "segmentation_module = SegmentationModule(net_encoder, net_decoder, criterion)\n",
        "segmentation_module.cuda()\n",
        "\n",
        "optimizer = optim.Adam(edgenet.parameters(), lr=0.0001)\n",
        "edgenet.apply(init_weights)\n",
        "train_model([edgenet, segmentation_module], train_loader, optimizer, criterion, epochs=20)\n",
        "show_test(test_model(edgenet, test_loader)[0])\n",
        "show_test(test_model(edgenet, test_loader)[1])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train_edited.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfbeYKQD_pO0",
        "colab_type": "code",
        "outputId": "8bba182d-e55a-4846-f29e-42bb9c913a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!python train_edited.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading weights for net_encoder\n",
            "Loading weights for net_decoder\n",
            "Traceback (most recent call last):\n",
            "  File \"train_edited.py\", line 238, in <module>\n",
            "    train_model([edgenet, segmentation_module], train_loader, optimizer, criterion, epochs=20)\n",
            "  File \"train_edited.py\", line 145, in train_model\n",
            "    outputs_obj = objectnet(y_descreen)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 489, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/ObjectNet/models/models.py\", line 35, in forward\n",
            "    pred = self.decoder(self.encoder(feed_dict['img_data'], return_feature_maps=True))\n",
            "IndexError: too many indices for tensor of dimension 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4UCbowuaHlg",
        "colab_type": "text"
      },
      "source": [
        "# Make sure using PYTORCH=1.0.x to run this code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3WYfu51aTB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "pip uninstall pytorch torchvision -y\n",
        "pip uninstall torch -y\n",
        "pip uninstall torch -y  # yes twice\n",
        "\n",
        "pip install https://download.pytorch.org/whl/cu100/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n",
        "pip install torchvision\n",
        "\n",
        "cd semantic-segmentation-pytorch/\n",
        "chmod +x demo_test.sh\n",
        "./demo_test.sh\n",
        "cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LzqDhjYGCxB",
        "colab_type": "code",
        "outputId": "6fbaf544-b851-4fd2-f9de-5d263ad09fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import torch\n",
        "print(torch.__version__, 'Do not consider this number. Python is telling bullshit!')\n",
        "import sys\n",
        "print(sys.version)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0 Do not consider this number. Python is telling bullshit!\n",
            "3.6.7 (default, Oct 22 2018, 11:32:17) \n",
            "[GCC 8.2.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm-cbF393brw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}