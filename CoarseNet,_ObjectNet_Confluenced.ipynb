{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoarseNet, ObjectNet Confluenced",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4UCbowuaHlg",
        "colab_type": "text"
      },
      "source": [
        "# Make sure using PYTORCH=1.0.x to run this code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3WYfu51aTB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "pip uninstall pytorch torchvision -y\n",
        "pip uninstall torch -y\n",
        "pip uninstall torch -y  # yes twice\n",
        "\n",
        "pip install https://download.pytorch.org/whl/cu100/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n",
        "pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LzqDhjYGCxB",
        "colab_type": "code",
        "outputId": "f7fbde9b-70dc-4fb5-e54f-fa10ad9488ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "import sys\n",
        "print(sys.version)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.1.post2 Do not consider this number. Python is telling bullshit!\n",
            "3.6.7 (default, Oct 22 2018, 11:32:17) \n",
            "[GCC 8.2.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgR92W3p27Mk",
        "colab_type": "text"
      },
      "source": [
        "# Clone required repos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pjdVhUccsEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/Nikronic/ObjectNet.git\n",
        "% cd ObjectNet/\n",
        "! git clone https://github.com/Nikronic/CoarseNet.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igG6DhnfZq6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! git clone https://github.com/Nikronic/EdgeNet.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P85rMZuIjryX",
        "colab_type": "text"
      },
      "source": [
        "# Download pretrained weights of ObjectNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM2ZLgsajubU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "\n",
        "# Image and model names\n",
        "TEST_IMG=ADE_val_00001519.jpg\n",
        "MODEL_PATH=baseline-resnet101dilated-ppm_deepsup\n",
        "RESULT_PATH=./\n",
        "\n",
        "ENCODER=$MODEL_PATH/encoder_epoch_25.pth\n",
        "DECODER=$MODEL_PATH/decoder_epoch_25.pth\n",
        "\n",
        "# Download model weights and image\n",
        "if [ ! -e $MODEL_PATH ]; then\n",
        "  mkdir $MODEL_PATH\n",
        "fi\n",
        "if [ ! -e $ENCODER ]; then\n",
        "  wget -P $MODEL_PATH http://sceneparsing.csail.mit.edu/model/pytorch/$ENCODER\n",
        "fi\n",
        "if [ ! -e $DECODER ]; then\n",
        "  wget -P $MODEL_PATH http://sceneparsing.csail.mit.edu/model/pytorch/$DECODER\n",
        "fi\n",
        "if [ ! -e $TEST_IMG ]; then\n",
        "  wget -P $RESULT_PATH http://sceneparsing.csail.mit.edu/data/ADEChallengeData2016/images/validation/$TEST_IMG\n",
        "fi\n",
        "\n",
        "# Inference\n",
        "python3 -u test.py \\\n",
        "  --model_path $MODEL_PATH \\\n",
        "  --test_imgs $TEST_IMG \\\n",
        "  --suffix '_epoch_25.pth' \\\n",
        "  --arch_encoder resnet101dilated \\\n",
        "  --arch_decoder ppm_deepsup \\\n",
        "  --fc_dim 2048 \\\n",
        "  --result $RESULT_PATH\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcPaUtSBWVsr",
        "colab_type": "text"
      },
      "source": [
        "# SubModules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd-o6T7LWU4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %% libraries\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class CoarseLoss(nn.Module):\n",
        "    def __init__(self, w1=50, w2=1):\n",
        "        \"\"\"\n",
        "        A weighted sum of pixel-wise L1 loss and sum of L2 loss of Gram matrices.\n",
        "\n",
        "        :param w1: weight of L1  (pixel-wise)\n",
        "        :param w2: weight of L2 loss (Gram matrix)\n",
        "        \"\"\"\n",
        "        super(CoarseLoss, self).__init__()\n",
        "        self.w1 = w1\n",
        "        self.w2 = w2\n",
        "        self.l1 = nn.L1Loss(reduction='mean')\n",
        "        self.l2 = nn.MSELoss(reduction='sum')\n",
        "\n",
        "    # reference: https://github.com/pytorch/tutorials/blob/master/advanced_source/neural_style_tutorial.py\n",
        "    @staticmethod\n",
        "    def gram_matrix(mat):\n",
        "        \"\"\"\n",
        "        Return Gram matrix\n",
        "\n",
        "        :param mat: A matrix  (a=batch size(=1), b=number of feature maps,\n",
        "        (c,d)=dimensions of a f. map (N=c*d))\n",
        "        :return: Normalized Gram matrix\n",
        "        \"\"\"\n",
        "        a, b, c, d = mat.size()\n",
        "        features = mat.view(a * b, c * d)\n",
        "        gram = torch.mm(features, features.t())\n",
        "        return gram.div(a * b * c * d)\n",
        "\n",
        "    def forward(self, y, y_pred):\n",
        "        loss = self.w1 * self.l1(y, y_pred) + \\\n",
        "               self.w2 * self.l2(self.gram_matrix(y), self.gram_matrix(y_pred))\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMDACC1BWvJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %% libraries\n",
        "import PIL.Image as Image\n",
        "import numpy.matlib\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "# %% functions\n",
        "dithMat =[\n",
        "    # 8x8 sprial\n",
        "    [[62, 58, 45, 41, 37, 49, 53, 61],\n",
        "     [54, 34, 25, 21, 17, 29, 33, 57],\n",
        "     [ 50, 30, 13,  9,  5, 12, 24, 44],\n",
        "     [ 38, 18,  6,  1,  0,  8, 20, 40],\n",
        "     [42, 22, 10, 2, 3, 4, 16, 36],\n",
        "     [46, 26, 14, 7, 11, 15, 28, 48],\n",
        "     [59, 35, 31, 19, 23, 27, 32, 52],\n",
        "     [ 63, 55, 51, 39, 43, 47, 56, 60]],\n",
        "    # 8x8 dispresed\n",
        "    [[ 1, 30, 8, 28, 2, 29, 7, 27],\n",
        "     [ 17, 9, 24, 16, 18, 10, 23, 15],\n",
        "     [ 5, 25, 3, 32, 6, 26, 4, 31],\n",
        "     [ 21, 13, 19, 11, 22, 14, 20, 12],\n",
        "     [ 2, 29, 7, 27, 1, 30, 8, 28],\n",
        "     [ 18, 10, 23, 15, 17, 9, 24, 16],\n",
        "     [ 6, 26, 4, 31, 5, 25, 3, 32],\n",
        "     [ 22, 14, 20, 12, 21, 13, 19, 11]],\n",
        "    # 8X8 octa_dot\n",
        "    [[ 45, 17, 25, 37, 47, 19, 27, 39],\n",
        "     [ 49, 1, 9, 57, 51, 3, 11, 59],\n",
        "     [ 29, 33, 41, 21, 31, 35, 43, 23],\n",
        "     [ 13, 61, 53, 5, 15, 63, 55, 7],\n",
        "     [ 48, 20, 28, 40, 46, 18, 26, 38],\n",
        "     [ 52, 4, 12, 60, 50, 2, 10, 58],\n",
        "     [ 32, 36, 44, 24, 30, 34, 42, 22],\n",
        "     [ 16, 64, 56, 8, 14, 62, 54, 6]],\n",
        "    # 5x5 diamond\n",
        "    [[ 5, 118, 160, 58, 17],\n",
        "     [ 48, 201, 232, 170, 99],\n",
        "     [ 129, 211, 252, 242, 150],\n",
        "     [ 89, 191, 221, 181, 68],\n",
        "     [ 38, 78, 140, 108, 27]],\n",
        "    # 5x5 clockwise sprial\n",
        "    [[3, 10, 16, 11, 4],\n",
        "     [ 9, 20, 21, 17, 12],\n",
        "     [ 15, 24, 25, 22, 13],\n",
        "     [ 8, 19, 23, 18, 5],\n",
        "     [ 2, 7, 14, 6, 1]],\n",
        "    # 4x4 ordered \n",
        "    [[ 5, 9, 6, 10],\n",
        "     [ 13, 1, 14, 2],\n",
        "     [ 7 ,11, 4, 8],\n",
        "     [ 15, 3, 12, 0]],\n",
        "]\n",
        "\n",
        "\n",
        "def get_resDmat(channel_size,dithMat):\n",
        "    newSzY,newSzX = channel_size[1],channel_size[0]\n",
        "    minDmat = min(min(dithMat))\n",
        "    maxDmat = max(max(dithMat))\n",
        "    nbOfIntervals = maxDmat-minDmat+2\n",
        "    singleInterval = 255/nbOfIntervals\n",
        "    scaledDithMat = np.multiply(np.subtract(dithMat , minDmat+1),singleInterval)\n",
        "    scaledDithMat = scaledDithMat.astype(int)\n",
        "\n",
        "\n",
        "    dmatSzY, dmatSzX = len(scaledDithMat),len(scaledDithMat[0])\n",
        "    nX = math.ceil(newSzX / dmatSzX) \n",
        "    nY = math.ceil(newSzY / dmatSzY)\n",
        "    resDmat = np.matlib.repmat(scaledDithMat.astype(int), nY, nX)[:newSzY,:newSzX]\n",
        "    return resDmat\n",
        "\n",
        "\n",
        "def generate_halftone(im):\n",
        "    cmyk_im = im.convert('CMYK')\n",
        "    dithMat_sample = dithMat[random.randint(0, len(dithMat) - 1)]\n",
        "    cmyk = cmyk_im.split()\n",
        "    angles = [[ 15, 45, 0, 75],\n",
        "              [ 45, 15, 0, 75],\n",
        "              [ 0, 0, 0, 0]]\n",
        "\n",
        "    angles = angles[random.randint(0, len(angles) - 1)]\n",
        "    if cmyk[0] == cmyk[1] == cmyk[2] :\n",
        "        angles = angles[:1]*4\n",
        "    dots = []\n",
        "    for x,i in enumerate(cmyk):\n",
        "        channel_Rotation = i.rotate(angles[x], expand=1)\n",
        "        channel = np.asarray(channel_Rotation) > get_resDmat(channel_Rotation.size,dithMat_sample)\n",
        "        channel = Image.fromarray((channel * 255).astype('uint8')).convert('L').rotate(-angles[x], expand=1)\n",
        "        # https://stackoverflow.com/questions/27622834/write-numpy-ndarray-to-image\n",
        "        # reason of casting to 'uint8'\n",
        "        w,h = channel.size\n",
        "        im_x,im_y = i.size\n",
        "        x1 = (w-im_x)/2\n",
        "        y1 = (h-im_y)/2\n",
        "        channel = channel.crop((x1, y1, x1+im_x, y1+im_y))\n",
        "        dots.append(channel)\n",
        "    \n",
        "    halftoned_im = Image.merge('CMYK',dots)\n",
        "    return halftoned_im.convert('RGB')\n",
        "\n",
        "\n",
        "# %% test\n",
        "# im = Image.open('data/Places365_val_00000001.jpg')\n",
        "# imh = generate_halftone(im)\n",
        "# imh.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T36wsF1Wfhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from PIL import Image\n",
        "from skimage import feature, color\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import tarfile\n",
        "import io\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# from utils.Halftone.halftone import generate_halftone\n",
        "\n",
        "\n",
        "class PlacesDataset(Dataset):\n",
        "    def __init__(self, txt_path='filelist.txt', img_dir='data', transform=None):\n",
        "        \"\"\"\n",
        "                Initialize data set as a list of IDs corresponding to each item of data set\n",
        "\n",
        "                :param img_dir: path to image files as a uncompressed tar archive\n",
        "                :param txt_path: a text file containing names of all of images line by line\n",
        "                :param transform: apply some transforms like cropping, rotating, etc on input image\n",
        "\n",
        "                :return a 3-value dict containing input image (y_descreen) as ground truth, input image X as halftone image\n",
        "                        and edge-map (y_edge) of ground truth image to feed into the network.\n",
        "                \"\"\"\n",
        "\n",
        "        df = pd.read_csv(txt_path, sep=' ', index_col=0)\n",
        "        self.img_names = df.index.values\n",
        "        self.txt_path = txt_path\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.to_tensor = ToTensor()\n",
        "        self.to_pil = ToPILImage()\n",
        "        self.get_image_selector = True if img_dir.__contains__('tar') else False\n",
        "        self.tf = tarfile.open(self.img_dir) if self.get_image_selector else None\n",
        "\n",
        "    def get_image_from_tar(self, name):\n",
        "        \"\"\"\n",
        "        Gets a image by a name gathered from file list csv file\n",
        "\n",
        "        :param name: name of targeted image\n",
        "        :return: a PIL image\n",
        "        \"\"\"\n",
        "        image = self.tf.extractfile(name)\n",
        "        image = image.read()\n",
        "        image = Image.open(io.BytesIO(image))\n",
        "        return image\n",
        "\n",
        "    def get_image_from_folder(self, name):\n",
        "        \"\"\"\n",
        "        gets a image by a name gathered from file list text file\n",
        "\n",
        "        :param name: name of targeted image\n",
        "        :return: a PIL image\n",
        "        \"\"\"\n",
        "\n",
        "        image = Image.open(os.path.join(self.img_dir, name))\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the length of data set using list of IDs\n",
        "\n",
        "        :return: number of samples in data set\n",
        "        \"\"\"\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Generate one item of data set. Here we apply our preprocessing things like halftone styles and\n",
        "        subtractive color process using CMYK color model, generating edge-maps, etc.\n",
        "\n",
        "        :param index: index of item in IDs list\n",
        "\n",
        "        :return: a sample of data as a dict\n",
        "        \"\"\"\n",
        "\n",
        "        if index == (self.__len__() - 1) and self.get_image_selector:  # close tarfile opened in __init__\n",
        "            self.tf.close()\n",
        "\n",
        "        if self.get_image_selector:  # note: we prefer to extract then process!\n",
        "            y_descreen = self.get_image_from_tar(self.img_names[index])\n",
        "        else:\n",
        "            y_descreen = self.get_image_from_folder(self.img_names[index])\n",
        "\n",
        "        # generate halftone image\n",
        "        X = generate_halftone(y_descreen)\n",
        "\n",
        "        # generate edge-map\n",
        "        y_edge = self.canny_edge_detector(y_descreen)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            X = self.transform(X)\n",
        "            y_descreen = self.transform(y_descreen)\n",
        "            y_edge = self.transform(y_edge)\n",
        "\n",
        "        sample = {'X': X,\n",
        "                  'y_descreen': y_descreen,\n",
        "                  'y_edge': y_edge}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def canny_edge_detector(self, image):\n",
        "        \"\"\"\n",
        "        Returns a binary image with same size of source image which each pixel determines belonging to an edge or not.\n",
        "\n",
        "        :param image: PIL image\n",
        "        :return: Binary numpy array\n",
        "        \"\"\"\n",
        "        if type(image) == torch.Tensor:\n",
        "            image = self.to_pil(image)\n",
        "        image = image.convert(mode='L')\n",
        "        image = np.array(image)\n",
        "        edges = feature.canny(image, sigma=1)  # TODO: the sigma hyper parameter value is not defined in the paper.\n",
        "        size = edges.shape[::-1]\n",
        "        databytes = np.packbits(edges, axis=1)\n",
        "        edges = Image.frombytes(mode='1', size=size, data=databytes)\n",
        "        return edges\n",
        "\n",
        "\n",
        "# https://discuss.pytorch.org/t/adding-gaussion-noise-in-cifar10-dataset/961/2\n",
        "class RandomNoise(object):\n",
        "    def __init__(self, p, mean=0, std=1):\n",
        "        self.p = p\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() <= self.p:\n",
        "            return img.clone().normal_(self.mean, self.std)\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbz5093j3XKX",
        "colab_type": "text"
      },
      "source": [
        "# TRAIN.py  - - - - - - - - - - - - - - - CoarseNet-ObjectNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AOMeaZG3cxJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "4acdd6d5-2eed-4df8-ea1b-ed8946e648ab"
      },
      "source": [
        "\n",
        "\n",
        "###########################\n",
        "# System libs\n",
        "import os\n",
        "import argparse\n",
        "from distutils.version import LooseVersion\n",
        "# Numerical libs\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy.io import loadmat\n",
        "# Our libs\n",
        "\n",
        "from models import ModelBuilder, SegmentationModule\n",
        "from utils import colorEncode\n",
        "from lib.nn import user_scattered_collate, async_copy_to\n",
        "from lib.utils import as_numpy\n",
        "import lib.utils.data as torchdata\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "###########################\n",
        "\n",
        "# %% import library\n",
        "from CoarseNet import CoarseNet\n",
        "from torchvision.transforms import Compose, ToPILImage, ToTensor, RandomResizedCrop, RandomRotation, \\\n",
        "    RandomHorizontalFlip\n",
        "# from CoarseNet.utils.preprocess import *\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "# from CoarseNet.utils.Loss import CoarseLoss\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.backends import cudnn\n",
        "\n",
        "#################################\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# %% define data sets and their loaders\n",
        "custom_transforms = Compose([\n",
        "    RandomResizedCrop(size=224, scale=(0.8, 1.2)),\n",
        "    RandomRotation(degrees=(-30, 30)),\n",
        "    RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    RandomNoise(p=0.5, mean=0, std=0.1)])\n",
        "\n",
        "train_dataset = PlacesDataset(txt_path='CoarseNet/filelist.txt',\n",
        "                              img_dir='CoarseNet/data',\n",
        "                              transform=custom_transforms)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=128,\n",
        "                          shuffle=True,\n",
        "                          num_workers=6,\n",
        "                          pin_memory= False)\n",
        "\n",
        "test_dataset = PlacesDataset(txt_path='CoarseNet/filelist.txt',\n",
        "                             img_dir='CoarseNet/data',\n",
        "                             transform=ToTensor())\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=128,\n",
        "                         shuffle=False,\n",
        "                         num_workers=0,\n",
        "                         pin_memory=False)\n",
        "\n",
        "\n",
        "# %% initialize network, loss and optimizer\n",
        "def init_weights(m):\n",
        "    \"\"\"\n",
        "    Initialize weights of layers using Kaiming Normal (He et al.) as argument of \"Apply\" function of\n",
        "    \"nn.Module\"\n",
        "\n",
        "    :param m: Layer to initialize\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
        "        m.bias.data.fill_(0.0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):  # reference: https://github.com/pytorch/pytorch/issues/12259\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def show_test(image_batch):\n",
        "    \"\"\"\n",
        "    Get a batch of images of torch.Tensor type and show them as a single gridded PIL image\n",
        "\n",
        "    :param image_batch: A Batch of torch.Tensor contain images\n",
        "    :return: An array of PIL images\n",
        "    \"\"\"\n",
        "    to_pil = ToPILImage()\n",
        "    fs = []\n",
        "    for i in range(len(image_batch)):\n",
        "        img = to_pil(image_batch[i].cpu())\n",
        "        fs.append(img)\n",
        "    x, y = fs[0].size\n",
        "    ncol = 3\n",
        "    nrow = 3\n",
        "    cvs = Image.new('RGB', (x * ncol, y * nrow))\n",
        "    for i in range(len(fs)):\n",
        "        px, py = x * int(i / nrow), y * (i % nrow)\n",
        "        cvs.paste((fs[i]), (px, py))\n",
        "    cvs.save('out.png', format='png')\n",
        "    cvs.show()\n",
        "    return fs\n",
        "  \n",
        "  \n",
        "  \n",
        "def test_model(net, data_loader):\n",
        "    \"\"\"\n",
        "    Return loss on test\n",
        "\n",
        "    :param net: The trained NN network\n",
        "    :param data_loader: Data loader containing test set\n",
        "    :return: Print loss value over test set in console\n",
        "    \"\"\"\n",
        "    \n",
        "    coarsenet = net[0]\n",
        "    objectnet = net[1]\n",
        "    \n",
        "    coarsenet.train()\n",
        "    objectnet.eval()\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            X = data['X']\n",
        "            y_d = data['y_descreen']\n",
        "            X = X.to(device)\n",
        "            y_d = y_d.to(device)\n",
        "            outputs = coarsenet(X)\n",
        "            output_obj=objectnet(outputs)\n",
        "            loss = criterion(output_obj, y_d)\n",
        "            running_loss += loss\n",
        "\n",
        "            print('loss: %.3f' % running_loss)\n",
        "    return outputs, output_obj\n",
        "  \n",
        "        \n",
        "def train_model(net, data_loader, optimizer, criterion, epochs=10):\n",
        "    \"\"\"\n",
        "    Train model\n",
        "\n",
        "    :param net: Parameters of defined neural network\n",
        "    :param data_loader: A data loader object defined on train data set\n",
        "    :param epochs: Number of epochs to train model\n",
        "    :param optimizer: Optimizer to train network\n",
        "    :param criterion: The loss function to minimize by optimizer\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    coarsenet = net[0]\n",
        "    objectnet = net[1]\n",
        "    \n",
        "    coarsenet.train()\n",
        "    objectnet.eval()\n",
        "    for epoch in range(epochs): \n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "            \n",
        "            X = data['X']\n",
        "            y_d = data['y_descreen']\n",
        "\n",
        "            X = X.to(device)\n",
        "            y_d = y_d.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = coarsenet(X)\n",
        "            \n",
        "            output_obj = objectnet(outputs)\n",
        "            \n",
        "            loss = criterion(output_obj, y_d)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            print(epoch + 1, ',', i + 1, 'loss:', running_loss)\n",
        "    print('Finished Training')    \n",
        "    \n",
        "\n",
        "    \n",
        "criterion = CoarseLoss(w1=50, w2=1)\n",
        "coarsenet = CoarseNet.CoarseNet().to(device)\n",
        "optimizer = optim.Adam(coarsenet.parameters(), lr=0.0001)\n",
        "coarsenet.apply(init_weights)\n",
        "\n",
        "builder = ModelBuilder()\n",
        "net_encoder = builder.build_encoder(\n",
        "    arch='resnet101dilated',\n",
        "    fc_dim=2048,\n",
        "    weights=os.path.join('baseline-resnet101dilated-ppm_deepsup', 'encoder' + '_epoch_25.pth'))\n",
        "net_decoder = builder.build_decoder(\n",
        "    arch='ppm_deepsup',\n",
        "    fc_dim=2048,\n",
        "    num_class=150,\n",
        "    weights=os.path.join('baseline-resnet101dilated-ppm_deepsup', 'decoder' + '_epoch_25.pth'),\n",
        "    use_softmax=True)\n",
        "\n",
        "    \n",
        "\n",
        "segmentation_module = SegmentationModule(net_encoder, net_decoder, criterion)\n",
        "segmentation_module.cuda()\n",
        "\n",
        "\n",
        "train_model([coarsenet, segmentation_module], train_loader, optimizer, criterion, epochs=20)\n",
        "show_test(test_model([coarsenet, segmentation_module], test_loader)[0])\n",
        "show_test(test_model([coarsenet, segmentation_module], test_loader)[1])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading weights for net_encoder\n",
            "Loading weights for net_decoder\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-b61428156383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoarsenet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation_module\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0mshow_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoarsenet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation_module\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0mshow_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoarsenet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation_module\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-b61428156383>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, data_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoarsenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0moutput_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjectnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ObjectNet/models/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feed_dict, segSize)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_deepsup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_feature_maps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_feature_maps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seg_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZclIkbleMOWa",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm-cbF393brw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir data\n",
        "! tar -xvf data.tar -C data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1WgsbtvKbHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ac9a4039-1b1c-48cc-c9a7-ff8d6a2e9c51"
      },
      "source": [
        "! pwd\n",
        "% cd .."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ObjectNet/CoarseNet\n",
            "/content/ObjectNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QlOpvpBMkmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8bf63b3-49f0-4a8d-9f62-277354e79751"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/ObjectNet'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRi2e53dXg1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}